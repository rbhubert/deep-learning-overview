{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1515765f",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "\n",
    "#### For the course Complete Tensorflow 2 and Keras Deeplearning Bootcamp\n",
    "\n",
    "In this project will be process the work of Charles Dickens and create a model capable of generate text based on the style of the author.\n",
    "\n",
    "We can obtained any free text from [Gutenberg](https://www.gutenberg.org/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001fe62",
   "metadata": {},
   "source": [
    "## Reading the data and creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dedad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02267432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER I.\n",
      "TREATS OF THE PLACE WHERE OLIVER TWIST WAS BORN AND OF THE\n",
      "CIRCUMSTANCES ATTENDING HIS BIRTH\n",
      "\n",
      "\n",
      "Among other public buildings in a certain town, which for many reasons\n",
      "it will be prudent to refrain from mentioning, and to which I will\n",
      "assign no fictitious name, there is one anciently common to most towns,\n",
      "great or small: to wit, a workhouse; and in this workhouse was born; on\n",
      "a day and date which I need not trouble myself to repeat, inasmuch as\n",
      "it can be of no possible consequence to t\n"
     ]
    }
   ],
   "source": [
    "# Reading the data\n",
    "text = open(\"data/dickens.txt\", \"r\", encoding=\"utf8\").read()\n",
    "\n",
    "# Let's print the first 500 characters\n",
    "print(text[:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bae8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9035263 characters in the text\n"
     ]
    }
   ],
   "source": [
    "# How many characters do we have\n",
    "print(f\"We have {len(text)} characters in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e2bb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '£',\n",
       " 'æ',\n",
       " 'ö',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a vocabulary with each character in the text.\n",
    "# Vocabulary is a list\n",
    "\n",
    "vocabulary = sorted(set(text))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57d5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary has 89 unique characters\n"
     ]
    }
   ],
   "source": [
    "# How long is our vocabulary\n",
    "print(f\"Our vocabulary has {len(vocabulary)} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a0276",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "We will create two maps: one from character to index in the vocabulary, and another from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31fda8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " '*': 8,\n",
       " ',': 9,\n",
       " '-': 10,\n",
       " '.': 11,\n",
       " '/': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '4': 17,\n",
       " '5': 18,\n",
       " '6': 19,\n",
       " '7': 20,\n",
       " '8': 21,\n",
       " '9': 22,\n",
       " ':': 23,\n",
       " ';': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " 'a': 55,\n",
       " 'b': 56,\n",
       " 'c': 57,\n",
       " 'd': 58,\n",
       " 'e': 59,\n",
       " 'f': 60,\n",
       " 'g': 61,\n",
       " 'h': 62,\n",
       " 'i': 63,\n",
       " 'j': 64,\n",
       " 'k': 65,\n",
       " 'l': 66,\n",
       " 'm': 67,\n",
       " 'n': 68,\n",
       " 'o': 69,\n",
       " 'p': 70,\n",
       " 'q': 71,\n",
       " 'r': 72,\n",
       " 's': 73,\n",
       " 't': 74,\n",
       " 'u': 75,\n",
       " 'v': 76,\n",
       " 'w': 77,\n",
       " 'x': 78,\n",
       " 'y': 79,\n",
       " 'z': 80,\n",
       " '£': 81,\n",
       " 'æ': 82,\n",
       " 'ö': 83,\n",
       " '—': 84,\n",
       " '‘': 85,\n",
       " '’': 86,\n",
       " '“': 87,\n",
       " '”': 88}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to obtain the index of a character\n",
    "\n",
    "character_to_index = {character: index for index, character in enumerate(vocabulary)}\n",
    "character_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b378ab38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '/',\n",
       "       '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
       "       'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
       "       'x', 'y', 'z', '£', 'æ', 'ö', '—', '‘', '’', '“', '”'], dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to obtain a character based on a index\n",
    "\n",
    "index_to_character = np.array(vocabulary)\n",
    "index_to_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2dcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to index de J: 35\n",
      "Index to character de 35: J\n"
     ]
    }
   ],
   "source": [
    "# Example of use\n",
    "\n",
    "print(f\"Character to index de J: {character_to_index['J']}\")\n",
    "print(f\"Index to character de 35: {index_to_character[35]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f464039",
   "metadata": {},
   "source": [
    "## Encoding the text\n",
    "\n",
    "Let's encode the text using the character_to_index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c88e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 28, 33, ..., 68, 59,  2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = np.array([character_to_index[character] for character in text])\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6f88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9035263 encoded characters in the text\n"
     ]
    }
   ],
   "source": [
    "# How many encoded characters do we have\n",
    "print(f\"We have {len(encoded_text)} encoded characters in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410648e",
   "metadata": {},
   "source": [
    "#### Let's show the text and its encoded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f911fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n to most towns,\\ngreat or small: to wit, a workhouse; and in this workhouse was born; on\\na day and date which I need not trouble myself to repeat, inasmuch as\\nit can be of no possible consequence to t'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[300:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a995cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68,  1, 74, 69,  1, 67, 69, 73, 74,  1, 74, 69, 77, 68, 73,  9,  0,\n",
       "       61, 72, 59, 55, 74,  1, 69, 72,  1, 73, 67, 55, 66, 66, 23,  1, 74,\n",
       "       69,  1, 77, 63, 74,  9,  1, 55,  1, 77, 69, 72, 65, 62, 69, 75, 73,\n",
       "       59, 24,  1, 55, 68, 58,  1, 63, 68,  1, 74, 62, 63, 73,  1, 77, 69,\n",
       "       72, 65, 62, 69, 75, 73, 59,  1, 77, 55, 73,  1, 56, 69, 72, 68, 24,\n",
       "        1, 69, 68,  0, 55,  1, 58, 55, 79,  1, 55, 68, 58,  1, 58, 55, 74,\n",
       "       59,  1, 77, 62, 63, 57, 62,  1, 34,  1, 68, 59, 59, 58,  1, 68, 69,\n",
       "       74,  1, 74, 72, 69, 75, 56, 66, 59,  1, 67, 79, 73, 59, 66, 60,  1,\n",
       "       74, 69,  1, 72, 59, 70, 59, 55, 74,  9,  1, 63, 68, 55, 73, 67, 75,\n",
       "       57, 62,  1, 55, 73,  0, 63, 74,  1, 57, 55, 68,  1, 56, 59,  1, 69,\n",
       "       60,  1, 68, 69,  1, 70, 69, 73, 73, 63, 56, 66, 59,  1, 57, 69, 68,\n",
       "       73, 59, 71, 75, 59, 68, 57, 59,  1, 74, 69,  1, 74])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[300:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8456e",
   "metadata": {},
   "source": [
    "## Creating Batches\n",
    "\n",
    "We need to understand how the text sequences are organized and shifted one character forward. We will use tensorflow datasets to create and shuffle batches.\n",
    "\n",
    "The batches are the training sequences that the model will use to learn and then generate new text. We have to determine how long those sequences should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a3c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER I.\n",
      "TREATS OF THE PLACE WHERE OLIVER TWIST WAS BORN AND OF THE\n",
      "CIRCUMSTANCES ATTENDING HIS BIRTH\n",
      "\n",
      "\n",
      "Among other public buildings in a certain town, which for many reasons\n",
      "it will be prudent to refrain from mentioning, and to which I will\n",
      "assign no fictitious name, there is one anciently common to most towns,\n",
      "great or small: to wit, a workhouse; and in this workhouse was born; on\n",
      "a day and date which I need not trouble myself to repeat, inasmuch as\n",
      "it can be of no possible consequence to the reader, in this stage of\n",
      "the business at all events; the item of mortality whose name is\n",
      "prefixed to the head of this chapter.\n",
      "\n",
      "For a long time after it was ushered into this world of sorrow and\n",
      "trouble, by the parish surgeon, it remained a matter of considerable\n",
      "doubt whether the child would survive to bear any name at all; in which\n",
      "case it is somewhat more than probable that these memoirs would never\n",
      "have appeared; or, if they had, that being comprised within a couple of\n",
      "pages, they would have possessed the inestimable merit of being the\n",
      "most concise and faithful specimen of biography, extant in the\n",
      "literature of any age or country.\n",
      "\n",
      "Although I am not disposed to maintain that the being born in a\n",
      "workhouse, is in itself the most fortunate and enviable circumstance\n",
      "that can possibly befall a human being, I do mean to say that in this\n",
      "particular instance, it was the best thing for Oliver Twist that could\n",
      "by possibility have occurred. The fact is, that there was considerable\n",
      "difficulty in inducing Oliver to take upon himself the office of\n",
      "respiration,—a troublesome practice, but one which custom has rendered\n",
      "necessary to our easy existence; and for some time he lay gasping on a\n",
      "little flock mattress, rather unequally poised between this world and\n",
      "the next: the balance being decidedly in favour of the latter. Now, if,\n",
      "during this brief period, Oliver had been surrounded by careful\n",
      "grandmothers, anxious aunts, experienced nurses, and doctors of\n",
      "profound wisdom, he would most \n"
     ]
    }
   ],
   "source": [
    "# Let's observe a part of the text and see if we can get a hold of how the data is structured.\n",
    "\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e47922d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paragraph length\n",
    "\n",
    "paragraph = '''\n",
    "Among other public buildings in a certain town, which for many reasons\n",
    "it will be prudent to refrain from mentioning, and to which I will\n",
    "assign no fictitious name, there is one anciently common to most towns,\n",
    "great or small: to wit, a workhouse; and in this workhouse was born; on\n",
    "a day and date which I need not trouble myself to repeat, inasmuch as\n",
    "it can be of no possible consequence to the reader, in this stage of\n",
    "the business at all events; the item of mortality whose name is\n",
    "prefixed to the head of this chapter.\n",
    "'''\n",
    "\n",
    "len(paragraph)\n",
    "\n",
    "# A paragraph (or maybe half of it) should help the model to understand how Dickens writes, \n",
    "# since each paragraph seems to be similar in style and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02194ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35997"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define our sequence length as 250\n",
    "sequence_length = 250\n",
    "\n",
    "# How many sequences we have in the text\n",
    "# The plus one is because of the zero indexing\n",
    "total_number_sequences = len(text) // (sequence_length + 1)\n",
    "total_number_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fc39b",
   "metadata": {},
   "source": [
    "Let's create the training sequences using the tensorflow Dataset object. We need to use our encoded_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20d7a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "character_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763c8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --  \n",
      "28 -- C\n",
      "33 -- H\n",
      "26 -- A\n",
      "41 -- P\n",
      "45 -- T\n",
      "30 -- E\n",
      "43 -- R\n",
      "1 --  \n",
      "34 -- I\n"
     ]
    }
   ],
   "source": [
    "# This TensorSliceDataset object allows us to create a dataset with atmost x number of characters, as in the example below.\n",
    "\n",
    "for item in character_dataset.take(10): # A dataset with 10 characters\n",
    "    x = item.numpy()\n",
    "    print(f\"{x} -- {index_to_character[x]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "087e22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create sequences using this TensorSliceDataset object method \"batch\"\n",
    "# The batch method combines consecutive elements of the dataset into batches.\n",
    "\n",
    "# --- plus 1 because zero indexing\n",
    "# --- drop_remainer in case that the last part of the dataset has not batch_size elements to conform a batch.\n",
    "sequences = character_dataset.batch(batch_size=sequence_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe9741",
   "metadata": {},
   "source": [
    "Let's create a function that allow us to have the input text and the output text. This output text (target) will be the input text one character shifted one forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e801a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_target(sequence):\n",
    "    input_text = sequence[:-1]       # Hello, my nam\n",
    "    target_text = sequence[1:]       # ello, my name\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aab4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can have our final dataset using this function.\n",
    "dataset = sequences.map(create_sequence_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fabd1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 28 33 26 41 45 30 43  1 34 11  0 45 43 30 26 45 44  1 40 31  1 45 33\n",
      " 30  1 41 37 26 28 30  1 48 33 30 43 30  1 40 37 34 47 30 43  1 45 48 34\n",
      " 44 45  1 48 26 44  1 27 40 43 39  1 26 39 29  1 40 31  1 45 33 30  0 28\n",
      " 34 43 28 46 38 44 45 26 39 28 30 44  1 26 45 45 30 39 29 34 39 32  1 33\n",
      " 34 44  1 27 34 43 45 33  0  0  0 26 67 69 68 61  1 69 74 62 59 72  1 70\n",
      " 75 56 66 63 57  1 56 75 63 66 58 63 68 61 73  1 63 68  1 55  1 57 59 72\n",
      " 74 55 63 68  1 74 69 77 68  9  1 77 62 63 57 62  1 60 69 72  1 67 55 68\n",
      " 79  1 72 59 55 73 69 68 73  0 63 74  1 77 63 66 66  1 56 59  1 70 72 75\n",
      " 58 59 68 74  1 74 69  1 72 59 60 72 55 63 68  1 60 72 69 67  1 67 59 68\n",
      " 74 63 69 68 63 68 61  9  1 55 68 58  1 74 69  1 77 62 63 57 62  1 34  1\n",
      " 77 63 66 66  0 55 73 73 63 61]\n",
      "\n",
      "\n",
      " CHAPTER I.\n",
      "TREATS OF THE PLACE WHERE OLIVER TWIST WAS BORN AND OF THE\n",
      "CIRCUMSTANCES ATTENDING HIS BIRTH\n",
      "\n",
      "\n",
      "Among other public buildings in a certain town, which for many reasons\n",
      "it will be prudent to refrain from mentioning, and to which I will\n",
      "assig\n",
      "\n",
      "Target\n",
      "[28 33 26 41 45 30 43  1 34 11  0 45 43 30 26 45 44  1 40 31  1 45 33 30\n",
      "  1 41 37 26 28 30  1 48 33 30 43 30  1 40 37 34 47 30 43  1 45 48 34 44\n",
      " 45  1 48 26 44  1 27 40 43 39  1 26 39 29  1 40 31  1 45 33 30  0 28 34\n",
      " 43 28 46 38 44 45 26 39 28 30 44  1 26 45 45 30 39 29 34 39 32  1 33 34\n",
      " 44  1 27 34 43 45 33  0  0  0 26 67 69 68 61  1 69 74 62 59 72  1 70 75\n",
      " 56 66 63 57  1 56 75 63 66 58 63 68 61 73  1 63 68  1 55  1 57 59 72 74\n",
      " 55 63 68  1 74 69 77 68  9  1 77 62 63 57 62  1 60 69 72  1 67 55 68 79\n",
      "  1 72 59 55 73 69 68 73  0 63 74  1 77 63 66 66  1 56 59  1 70 72 75 58\n",
      " 59 68 74  1 74 69  1 72 59 60 72 55 63 68  1 60 72 69 67  1 67 59 68 74\n",
      " 63 69 68 63 68 61  9  1 55 68 58  1 74 69  1 77 62 63 57 62  1 34  1 77\n",
      " 63 66 66  0 55 73 73 63 61 68]\n",
      "\n",
      "\n",
      "CHAPTER I.\n",
      "TREATS OF THE PLACE WHERE OLIVER TWIST WAS BORN AND OF THE\n",
      "CIRCUMSTANCES ATTENDING HIS BIRTH\n",
      "\n",
      "\n",
      "Among other public buildings in a certain town, which for many reasons\n",
      "it will be prudent to refrain from mentioning, and to which I will\n",
      "assign\n"
     ]
    }
   ],
   "source": [
    "# Let's see how this looks like\n",
    "\n",
    "for input_text, target_text in dataset.take(1):\n",
    "    print(input_text.numpy()) # How our network sees the dataset\n",
    "    print(\"\\n\")\n",
    "    print(\"\".join(index_to_character[input_text.numpy()])) # what the dataset look to us\n",
    "    print(\"\\nTarget\")\n",
    "    print(target_text.numpy()) # target that our network sees\n",
    "    print(\"\\n\")\n",
    "    print(\"\".join(index_to_character[target_text.numpy()])) # target that we see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92c0c2",
   "metadata": {},
   "source": [
    "### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e9e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size is the number of training examples utilized in one iteration\n",
    "batch_size = 128\n",
    "\n",
    "# Let's shuffle the batches so the model does not learn from a particular ordering of the text\n",
    "buffer_size = 10000 # to avoid to have all the batches in memory at the same time\n",
    "\n",
    "# This method means - take 10000 elements and shuffle them\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee03a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(128, 250), dtype=tf.int32, name=None), TensorSpec(shape=(128, 250), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input sequences -- 128 sequences of sequence_length length # 250\n",
    "# target sequences -- 128 and sequence_length length # 250\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abf859",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We need to define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8507db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first one is the vocabulary size, defined by the vocabulary\n",
    "vocabulary_size = len(vocabulary)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab709f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding_dimensions variable is for our Embedding layer. Ideally, an embedding captures some of the semantics of the \n",
    "# input by placing semantically similar inputs close together in the embedding space.\n",
    "\n",
    "# This number should be in the same scale of the vocabulary size\n",
    "embedding_dimensions = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d66ec4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a single layer with a lot of neurons\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a449400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# We use sparse_categorical_crossentropy because we want a single category. \n",
    "# We might use this loss function when:\n",
    "#    our classes are mutually exclusive\n",
    "#    or the number of categories is large so the prediction output becomes overwhelming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a136659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define our function for sparse categorical loss because we need to define this \"from_logits\" as True\n",
    "# We need this parameter to be true because use one-hot encoding\n",
    "\n",
    "def sparse_categorical_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45cb3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the function what will take \n",
    "# - the vocabulary size\n",
    "# - the number of rnn neurons\n",
    "# - and the batch size\n",
    "\n",
    "def create_model(vocabulary_size, rnn_neurons, batch_size):\n",
    "    \n",
    "    # Creation of the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # our first layer is an Embedding layer\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dimensions, batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # our second (hidden) layer is a GRU (RNN) layer\n",
    "    #   the return sequences will return the last output\n",
    "    #   the stateful means that the states computed for the samples in one batch will be reused as initial states for the \n",
    "    # samples in the next batch\n",
    "    #   \"glorot_uniform\" because it performs better\n",
    "    model.add(GRU(units=rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"))\n",
    "    \n",
    "    # output layer with vocabulary_size neurons, since we need the output to be one value of the vocabulary.\n",
    "    model.add(Dense(vocabulary_size))\n",
    "    \n",
    "    # Compilation of the model\n",
    "    model.compile(optimizer=\"adam\", loss=sparse_categorical_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94966a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (128, None, 64)           5696      \n",
      "                                                                 \n",
      " gru (GRU)                   (128, None, 1026)         3361176   \n",
      "                                                                 \n",
      " dense (Dense)               (128, None, 89)           91403     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,458,275\n",
      "Trainable params: 3,458,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creation of the model\n",
    "\n",
    "model = create_model(vocabulary_size=vocabulary_size, rnn_neurons=rnn_neurons, batch_size=batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8a03f",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f6ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take a while... \n",
    "# I trained in Google Colab making use of a GPU.\n",
    "\n",
    "# model.fit(dataset, epochs=30) \n",
    "\n",
    "# I save the model result in /data/model_dickens.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4928c5",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5cb1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model to be able to use it now\n",
    "model = create_model(vocabulary_size=vocabulary_size, rnn_neurons=rnn_neurons, batch_size=1)\n",
    "model.load_weights(\"data/model_dickens.h5\") # load weights based on the trained model\n",
    "\n",
    "# we build the model using the input shape\n",
    "# We set 1, None because we will have one array of an unknown number of seed characters\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6835e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 64)             5696      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1026)           3361176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 89)             91403     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,458,275\n",
      "Trainable params: 3,458,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55366",
   "metadata": {},
   "source": [
    "## Generation of new text\n",
    "\n",
    "Let's create a function that allows us to create new text using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b369282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function generate_text will use the following parameters\n",
    "# - model: trained model to generate text\n",
    "# - start_seed: initial seed (text -- string form)\n",
    "# - gen_size: number of characters to generate\n",
    "# - temperature: this parameter effects randomness in our resulting text.\n",
    "    # higher temperature/probability => lesss surprising / more expected\n",
    "    # lower temperature => more surprising / less expected\n",
    "\n",
    "def generate_text(model, start_seed: str, gen_size=100, temperature=1.0):\n",
    "\n",
    "    # number of characters to generate\n",
    "    num_generate = gen_size\n",
    "\n",
    "    # vectorizing the starting seed text (using our map from character to index)\n",
    "    input_eval = [character_to_index[s] for s in start_seed]\n",
    "\n",
    "    # expand our vector to match the batch format shape\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # this list will hold the generated text\n",
    "    text_generated = []\n",
    "    temperature = temperature\n",
    "\n",
    "    # batch_size is equals to 1\n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "\n",
    "        # generation of predictions using input_eval\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # remove the batch shape dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # use a categorical distribution to select the next character\n",
    "        # so we don't always end up choosing the character with the highest probability\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # modify the input eval to take the predicted character an use it as the next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        # transform back to character and save it in the text_generated list\n",
    "        text_generated.append(index_to_character[predicted_id])\n",
    "\n",
    "    return (start_seed + \"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b41ec066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear, had not yet dropped\n",
      "upon her from address her christen Mr. Weller’s pious face appeared to descinate\n",
      "me what I suffered and hold the money out.’\n",
      "\n",
      "‘Have you any degraded crease condescended to grom the circumbsy of his\n",
      "sister’s, and their lips, shows: a fleshy cavern. Such things come, and all\n",
      "was in them, and to accept him in my education and pleasure), looking at him with\n",
      "that persict to come off.\n",
      "\n",
      "‘Do; boy--’\n",
      "\n",
      "Saturiate to slighted Riderhood’s waist, the whole of the united little children proceeded, and rendered\n",
      "the coachman to complete the habit of satisfactory conceition, I\n",
      "went downstairs into my own head and red immense. Cruzz too much\n",
      "scowning away.’\n",
      "\n",
      "‘The dead sole hearts, poor little fowl!--and it be my vivid--now my\n",
      "daughter--’\n",
      "\n",
      "Here Mr Milvoyth as I sits to grop was foolish.\n",
      "\n",
      "In short, the idea of all the sight twist into the house, or all that\n",
      "supervite of its cause of very large or four bill bargained; and the\n",
      "paper will purchases she would have done to him--very quick \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"dear\", gen_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
